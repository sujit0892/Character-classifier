{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import ndimage\n",
    "import pickle as pickle\n",
    "from PIL import Image\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '.'\n",
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "test_folders = ['./notMNIST_small/A', './notMNIST_small/B',\n",
    "'./notMNIST_small/C', './notMNIST_small/D',\n",
    "'./notMNIST_small/E', './notMNIST_small/F',\n",
    "'./notMNIST_small/G', './notMNIST_small/H',\n",
    "'./notMNIST_small/I', './notMNIST_small/J']\n",
    "\n",
    "train_folders = ['./notMNIST_large_v2/A', './notMNIST_large_v2/B',\n",
    "'./notMNIST_large_v2/C', './notMNIST_large_v2/D',\n",
    "'./notMNIST_large_v2/E', './notMNIST_large_v2/F',\n",
    "'./notMNIST_large_v2/G', './notMNIST_large_v2/H',\n",
    "'./notMNIST_large_v2/I', './notMNIST_large_v2/J']\n",
    "\n",
    "image_size = 28 #pixel eidth and height\n",
    "pixel_depth = 255.0\n",
    "num_of_labels = 10\n",
    "batch_size = 128\n",
    "no_of_neurons = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_letter(folder,min_num_images):\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files),image_size,image_size),dtype= np.float32)\n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder,image)\n",
    "        image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                      pixel_depth/2)/pixel_depth\n",
    "        dataset[num_images,:,:] = image_data\n",
    "        num_images +=1\n",
    "    dataset = dataset[0:num_images,:,:]\n",
    "    if(num_images< min_num_images):\n",
    "        print('few_imges than expected')\n",
    "    print('dataset tensor:', dataset.shape)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_pickle(data_folders, min_num_imagesper_class,force = False):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder+\".pickle\"\n",
    "        dataset_names.append(set_filename)\n",
    "        if(os.path.exists(set_filename) and not force):\n",
    "            print(set_filename,\"already present\")\n",
    "        else:\n",
    "            print(set_filename)\n",
    "            dataset = load_letter(folder,100)\n",
    "            try:\n",
    "                with open(set_filename,'wb') as f:\n",
    "                    print(pickle.HIGHEST_PROTOCOL)\n",
    "                    pickle.dump(dataset,f,2)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data',set_filename,\":\",e)\n",
    "    return dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "        labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "    num_classes = len(pickle_files)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "    vsize_per_class = valid_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class+tsize_per_class\\\n",
    "    for label, pickle_file in enumerate(pickle_files):       \n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                letter_set = pickle.load(f)\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "                    valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "                    valid_labels[start_v:end_v] = label\n",
    "                    start_v += vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "                    \n",
    "                train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "                train_dataset[start_t:end_t, :, :] = train_letter\n",
    "                train_labels[start_t:end_t] = label\n",
    "                start_t += tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':', e)\n",
    "            raise\n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./notMNIST_large_v2/A.pickle already present\n",
      "./notMNIST_large_v2/B.pickle already present\n",
      "./notMNIST_large_v2/C.pickle already present\n",
      "./notMNIST_large_v2/D.pickle already present\n",
      "./notMNIST_large_v2/E.pickle already present\n",
      "./notMNIST_large_v2/F.pickle already present\n",
      "./notMNIST_large_v2/G.pickle already present\n",
      "./notMNIST_large_v2/H.pickle already present\n",
      "./notMNIST_large_v2/I.pickle already present\n",
      "./notMNIST_large_v2/J.pickle already present\n",
      "./notMNIST_small/A.pickle already present\n",
      "./notMNIST_small/B.pickle already present\n",
      "./notMNIST_small/C.pickle already present\n",
      "./notMNIST_small/D.pickle already present\n",
      "./notMNIST_small/E.pickle already present\n",
      "./notMNIST_small/F.pickle already present\n",
      "./notMNIST_small/G.pickle already present\n",
      "./notMNIST_small/H.pickle already present\n",
      "./notMNIST_small/I.pickle already present\n",
      "./notMNIST_small/J.pickle already present\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders,100)\n",
    "test_datasets = maybe_pickle(test_folders,100)\n",
    "train_size = 1000\n",
    "valid_size = 500\n",
    "test_size = 500\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "    train_datasets, train_size, valid_size)\n",
    "\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset and labels shape: (1000, 28, 28) (1000,)\n",
      "Validation dataset and labels shape: (500, 28, 28) (500,)\n",
      "Testing dataset and labels shape: (500, 28, 28) (500,)\n",
      "Compressed pickle size: 6280500\n"
     ]
    }
   ],
   "source": [
    "print('Training dataset and labels shape:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation dataset and labels shape:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing dataset and labels shape:', test_dataset.shape, test_labels.shape)\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)\n",
    "\n",
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "      'train_dataset': train_dataset,\n",
    "      'train_labels': train_labels,\n",
    "      'valid_dataset': valid_dataset,\n",
    "      'valid_labels': valid_labels,\n",
    "      'test_dataset': test_dataset,\n",
    "      'test_labels': test_labels,\n",
    "    }\n",
    "\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(dataset,labels):\n",
    "    dataset = dataset.reshape((-1,image_size*image_size)).astype(np.float32)\n",
    "    labels = (np.arange(num_of_labels)==labels[:,None]).astype(np.float32)\n",
    "    print(dataset.shape)\n",
    "    print(labels.shape)\n",
    "    return dataset,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset and labels shape: (1000, 28, 28) (1000,)\n",
      "Validation dataset and labels shape: (500, 784) (500, 10)\n",
      "Testing dataset and labels shape: (500, 28, 28) (500,)\n",
      "(1000, 784)\n",
      "(1000, 10)\n",
      "(500, 784)\n",
      "(500, 10)\n",
      "(500, 784)\n",
      "(500, 10)\n"
     ]
    }
   ],
   "source": [
    "with open('notMNIST.pickle','rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    training_dataset = save['train_dataset']\n",
    "    training_labels = save['train_labels']\n",
    "    validation_dataset = save['valid_dataset']\n",
    "    validation_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save\n",
    "\n",
    "print('Training dataset and labels shape:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation dataset and labels shape:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing dataset and labels shape:', test_dataset.shape, test_labels.shape)\n",
    "training_dataset,training_labels = reformat(training_dataset,training_labels)\n",
    "valid_dataset,valid_labels = reformat(validation_dataset,validation_labels)\n",
    "test_dataset,test_labels = reformat(test_dataset,test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n",
      "mini_batch_accu  0   12.5\n",
      "valid_accu  0   19.0\n",
      "mini_batch_accu  10   65.625\n",
      "valid_accu  10   57.2\n",
      "mini_batch_accu  20   73.4375\n",
      "valid_accu  20   58.6\n",
      "mini_batch_accu  30   90.625\n",
      "valid_accu  30   62.0\n",
      "mini_batch_accu  40   95.3125\n",
      "valid_accu  40   63.0\n",
      "mini_batch_accu  50   96.875\n",
      "valid_accu  50   62.0\n",
      "mini_batch_accu  60   99.21875\n",
      "valid_accu  60   61.0\n",
      "mini_batch_accu  70   98.4375\n",
      "valid_accu  70   62.6\n",
      "mini_batch_accu  80   96.09375\n",
      "valid_accu  80   62.4\n",
      "mini_batch_accu  90   97.65625\n",
      "valid_accu  90   62.2\n",
      "mini_batch_accu  100   99.21875\n",
      "valid_accu  100   61.2\n",
      "test  76.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape = (\n",
    "        batch_size,image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32,shape = (batch_size,\n",
    "                                                         num_of_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    w1 = tf.Variable(tf.truncated_normal(([image_size*image_size, \n",
    "                                           no_of_neurons])))\n",
    "    b1 = tf.Variable(tf.zeros([no_of_neurons]))    \n",
    "    w2 = tf.Variable(tf.truncated_normal([no_of_neurons,num_of_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_of_labels]))\n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset,w1)+b1)\n",
    "    logits =tf.matmul(hidden1,w2)+b2   \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels = tf_train_labels,logits = logits))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction =  tf.nn.softmax(\n",
    "            tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), w2) + b2)\n",
    "    test_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w2) + b2)\n",
    "\n",
    "num_steps = 101\n",
    "x = np.arange(num_steps)\n",
    "minibatch_acc = []\n",
    "validation_acc = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step*batch_size)%(train_labels.shape[0]-batch_size)\n",
    "        batch_data = training_dataset[offset:(batch_size+offset),:]\n",
    "        batch_label = training_labels[offset:(batch_size+offset),:]\n",
    "        feed_dicts = {tf_train_dataset:batch_data,tf_train_labels:batch_label}\n",
    "        _,l, predictions = sess.run([optimizer,loss,train_prediction],feed_dicts)\n",
    "        mini_batch_accu = accuracy(predictions,batch_label)\n",
    "        valid_accu = accuracy(valid_prediction.eval(),valid_labels)\n",
    "        if(step%10==0):\n",
    "            print(\"mini_batch_accu \",step,\" \",mini_batch_accu)\n",
    "            print(\"valid_accu \",step,\" \",valid_accu)\n",
    "            minibatch_acc.append(mini_batch_accu)\n",
    "            validation_acc.append(valid_accu)\n",
    "            t=[np.array(minibatch_acc)]\n",
    "            t.append(validation_acc)\n",
    "\n",
    "    print(\"test \", accuracy(test_prediction.eval(),test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
